{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f3cdf3",
   "metadata": {},
   "source": [
    "# Attention-based multiple instance learning (MIL) classifier of the zebrafish\n",
    "\n",
    "This notebook trains attention-based multiple instance learning classifier of the zebrafish embryos. The code allows training of a model with one of the three alternative backbones: MobileNetV2, ResNet50 and a shalllow convolutional network. After training, classifier's accuracy is evaluated and attention distribution is analyzed for each class. For evaluation of the attention given to the different parts of the fish, the code uses masks which denote the areas of main fish parts for images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from skimage.transform import resize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from tqdm.auto import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from sklearn import metrics\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "import scipy\n",
    "sns.set_style('white')\n",
    "from utils import create_dataset\n",
    "# set manual seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51410d2",
   "metadata": {},
   "source": [
    "### Getting the zebrafish data\n",
    "\n",
    "Download and unzip the training data (images and classes) into the subfolder `data`\n",
    "```\n",
    "wget https://zenodo.org/record/6651752/files/data.zip?download=1  -O data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "\n",
    "Which should result in the following \n",
    "\n",
    "```\n",
    "data\n",
    "├── fish_part_labels\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "├── training\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "└── validation\n",
    "    ├── DAPT\n",
    "    ├── her1;her7\n",
    "    ├── tbx6_fss\n",
    "    └── WT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot= Path('data')\n",
    "target_size = (450,900)\n",
    "\n",
    "epochs  = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c9c3d",
   "metadata": {},
   "source": [
    "### Loading the zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ad391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = create_dataset(dataroot/\"training\",   target_size=target_size, batchsize=16, shuffle=True,  augment=True)\n",
    "data_val   = create_dataset(dataroot/\"validation\", target_size=target_size, batchsize=16, shuffle=False, augment=False)\n",
    "\n",
    "print(data_train.class_name_to_id)\n",
    "print(data_train.class_id_to_name)\n",
    "\n",
    "# plot some example training and validation images\n",
    "n=5\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "for i in range(2*n):\n",
    "    _x = data_train.transform(image=data_train.images[0])['image']\n",
    "    plt.subplot(2, n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'Single augmented - {i} class = {data_train.class_id_to_name[data_train.labels[0]]}')\n",
    "     \n",
    "x,y = next(iter(data_train.data))\n",
    "plt.figure(figsize=(20,6))\n",
    "for i, (_x, _y) in enumerate(zip(x[:2*n],y[:2*n])):\n",
    "    plt.subplot(2,n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'Training - class = {data_train.class_id_to_name[int(_y)]} ({int(_y)})')\n",
    "\n",
    "x,y = next(iter(data_val.data))\n",
    "plt.figure(figsize=(20,6))\n",
    "for i, (_x, _y) in enumerate(zip(x[:2*n],y[:2*n])):\n",
    "    plt.subplot(2,n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'Validation - class = {data_train.class_id_to_name[int(_y)]} ({int(_y)})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e82e6",
   "metadata": {},
   "source": [
    "### Building and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape):\n",
    "    inp = tf.keras.Input(shape=shape)\n",
    "\n",
    "    model_pretrained = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=shape, include_top=False, weights='imagenet')\n",
    "    model_pretrained.trainable = False\n",
    "    features = model_pretrained(inp, training=False)\n",
    "    features = tf.keras.layers.Conv2D(64, 1, padding=\"same\", activation=\"relu\")(features)\n",
    "\n",
    "    # attention mask\n",
    "    # two small pointwise convolutions\n",
    "    attention = tf.keras.layers.Conv2D(64,1, padding=\"same\", activation=\"tanh\")(features)\n",
    "    attention = tf.keras.layers.Conv2D(1,1, padding=\"same\", activation=\"linear\")(attention)\n",
    "    # make sure that it sums to one...\n",
    "    shape = attention.get_shape().as_list()        \n",
    "    attention = tf.keras.layers.Flatten()(attention)\n",
    "    attention = tf.math.softmax(attention)\n",
    "    attention = tf.keras.layers.Reshape(shape[1:])(attention)\n",
    "    # multiply by normalized attention \n",
    "    x = tf.multiply(features, attention)\n",
    "    x = tf.math.reduce_sum(x, axis=(1,2)) #attention pooling\n",
    "    #x = tf.math.reduce_mean(features, axis=(1,2)) #no attention\n",
    "    model_attention = tf.keras.Model(inp, attention)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    out = tf.keras.layers.Dense(4,)(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    return model, model_attention \n",
    "\n",
    "model, model_attention = build_model(target_size+(3,))\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "#preparing the weights for balanced training\n",
    "counts_mean = np.mean(tuple(data_train.counts.values()))\n",
    "class_weights = dict((k, counts_mean/v) for k,v in data_train.counts.items())\n",
    "print(f'class weights: {class_weights}')\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00025, verbose = 1)\n",
    "\n",
    "checkpoint_folder = Path('checkpoints')\n",
    "checkpoint_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "checkpoint_filepath = checkpoint_folder/f\"MIL_classifier.h5\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = model_attention.predict(x[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b805d",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa06a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(data_train.data.repeat(4),\n",
    "                epochs=epochs,\n",
    "                validation_data=data_val.data,\n",
    "                class_weight=class_weights, callbacks=[model_checkpoint_callback])\n",
    "np.save(checkpoint_folder/f\"MIL_lr_{learning_rate:.4f}_epochs_{epochs}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef32392",
   "metadata": {},
   "source": [
    "### Loading the best-epoch weights of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e120ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71197d55",
   "metadata": {},
   "source": [
    "### Loading history and plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95393575",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.load(checkpoint_folder/f\"MIL_lr_{learning_rate:.4f}_epochs_{epochs}.npy\",allow_pickle='TRUE').item()\n",
    "\n",
    "acc = history[f'sparse_categorical_accuracy']\n",
    "val_acc = history[f'val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history[f'loss']\n",
    "val_loss = history[f'val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training and Validation Loss');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fbb50",
   "metadata": {},
   "source": [
    "### Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(data_val.labels)\n",
    "y_pred = np.argmax(model.predict(data_val.data), -1)\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    classes = tuple(data_val.class_id_to_name[i] for i in range(4))\n",
    "    matrix = metrics.confusion_matrix(y_true, y_pred) #rows - true, columns - predicted\n",
    "    matrix = matrix/np.sum(matrix, axis=-1, keepdims=True)\n",
    "    \n",
    "    df_cm = pd.DataFrame(matrix, index=classes, columns=classes)\n",
    "    # plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='1.3f')# font size\n",
    "    plt.ylabel(\"True class\")\n",
    "    plt.xlabel(\"Predicted class\") \n",
    "    plt.show()\n",
    "    accuracy = np.sum(np.diag(matrix))/np.sum(matrix)\n",
    "    print(f'Accuracy: {accuracy:.4f}') \n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191746c",
   "metadata": {},
   "source": [
    "# Apply the model to new images from a different microscope (`kings` subset)\n",
    "\n",
    "Put the `kings` images (in bmp format) in the subfolder `data/kings`, resulting in \n",
    "\n",
    "```\n",
    "data\n",
    "├── fish_part_labels\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "├── kings\n",
    "│   ├── DAPT\n",
    "│   └── WT\n",
    "├── training\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "└── validation\n",
    "    ├── DAPT\n",
    "    ├── her1;her7\n",
    "    ├── tbx6_fss\n",
    "    └── WT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = create_dataset(dataroot/'kings', augment=False, shuffle=False, target_size=target_size)\n",
    "\n",
    "x,y = next(iter(data_new.data))\n",
    "plt.figure(figsize=(20,5))\n",
    "for i, (_x, _y) in enumerate(zip(x[:2*n],y[:2*n])):\n",
    "    plt.subplot(2,n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'New image - class = {data_train.class_id_to_name[int(_y)]} ({int(_y)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5800e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(tuple(data_new.class_id_to_name[c] for c in data_new.labels))\n",
    "y_prob = tf.math.softmax(model.predict(data_new.data))\n",
    "y_pred = np.array(tuple(data_train.class_id_to_name[c] for c in np.argmax(y_prob, -1)))\n",
    "y_prob = np.max(y_prob, axis=-1)\n",
    "\n",
    "accuracy = np.mean(y_true==y_pred)\n",
    "print(f'Accuracy: {accuracy:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7ad2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (x, fname, y1, y2, prob) in enumerate(zip(data_new.images, data_new.filenames, y_true, y_pred, y_prob)):\n",
    "    if i%4==0:\n",
    "        plt.figure(figsize=(20,4))\n",
    "    ax = plt.subplot(1,4,i%4+1)\n",
    "    ax.imshow(x)\n",
    "    correct = y1==y2\n",
    "    ax.set_title(f'{\"correct\" if correct else \"wrong\"} ({y1} -> {y2} with prob {prob:.2f})', fontsize=10)\n",
    "    ax.title.set_color('green' if correct else 'red')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e608af",
   "metadata": {},
   "source": [
    "### Show a few examples of the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data_val.data))\n",
    "\n",
    "pred = np.argmax(model.predict(x), axis=-1)\n",
    "att = model_attention.predict(x)\n",
    "n_plot= 4\n",
    "plt.figure(figsize=(20,10))\n",
    "for i,(_x,_y,_p, _a) in enumerate(zip(x[:n_plot],y[:n_plot], pred[:n_plot], att[:n_plot])):\n",
    "    plt.subplot(2,n_plot,i+1)\n",
    "    plt.imshow(_x)\n",
    "    plt.title(f\"True: {data_val.class_id_to_name[int(_y)]}\")    \n",
    "    plt.subplot(2,n_plot,n_plot+i+1)\n",
    "\n",
    "    plt.imshow(_x[...,0])    \n",
    "    plt.imshow(resize(_a,_x.shape[:2])[:,:,0], alpha=.6, cmap = 'magma')\n",
    "    plt.title(f\"Pred: {data_val.class_id_to_name[int(_p)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca44312",
   "metadata": {},
   "source": [
    "### Calculating attention for all images and saving validation set images with attention overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981cb0a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_test = create_dataset(dataroot/\"validation\", target_size=target_size, batchsize=1, shuffle=False, augment=False)\n",
    "\n",
    "outfolder = Path('MIL_results')\n",
    "folder_parts = dataroot/\"fish_part_labels\"\n",
    "\n",
    "#saving of the predicted images\n",
    "out_folders = dict((k, outfolder/v) for k,v in data_test.class_id_to_name.items())\n",
    "for k,folder in out_folders.items():\n",
    "    folder.mkdir(exist_ok =True, parents=True)        \n",
    "\n",
    "for i, ((x,y),filename) in tqdm(enumerate(zip(data_test.data, data_test.filenames)), total=len(data_test.data)):\n",
    "    y_pred = np.argmax(model.predict(x,verbose=False), axis=-1)\n",
    "    y_true = int(y[0])\n",
    "    att = model_attention.predict(x,verbose=False)[0,:,:,0]\n",
    "    \n",
    "    fname_parts = Path(*Path(filename).parts[-2:])\n",
    "    parts = tifffile.imread(folder_parts/f'{fname_parts}.tiff')\n",
    "    resized_parts = resize(parts, (450,900), order=0) #everything is streched to the shape of the loaded image\n",
    "    parts_array_binarized = np.where(resized_parts > 0, 0, 1)\n",
    "\n",
    "    if i<10:\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(x[0])   \n",
    "        plt.imshow(resize(att,x[0].shape[:2]), alpha=0.4, cmap = 'magma')\n",
    "        plt.imshow(parts_array_binarized, alpha = 0.1)\n",
    "        plt.box(False)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.savefig(out_folders[y_true]/Path(filename).name, dpi = fig.dpi, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092109a4",
   "metadata": {},
   "source": [
    "### Calculating the per-class- and per-fish-part attention for images from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(image, mask, index, normalization = False):\n",
    "    if(image.shape != mask.shape):\n",
    "        raise Exception('Image and Mask should have the same shape', image.shape, mask.shape)\n",
    "\n",
    "    index_mask = (mask==index)\n",
    "    attention = np.sum(index_mask*image)\n",
    "    if normalization:\n",
    "        attention = attention / (np.sum(index_mask)+1e-10)\n",
    "    return attention\n",
    "\n",
    "\n",
    "Attention = np.empty((4, 6, 20)) #4 rows are the classes, 5 colums are different zebrafish parts\n",
    "#indices for zebrafish parts: (in the array as well the values on the zebrafish part paintings)\n",
    "#0 - background\n",
    "#1 - head\n",
    "#2 - trunk\n",
    "#3 - tail \n",
    "#4 - yolk\n",
    "#5 - yolk extension\n",
    "\n",
    "class_counter = np.zeros(4) #keeping track of how many images from a certain class there were so far\n",
    "normalization = 1\n",
    "\n",
    "\n",
    "for i, ((x,y),filename) in tqdm(enumerate(zip(data_test.data, data_test.filenames)), total=len(data_test.data)):\n",
    "    y_pred = np.argmax(model.predict(x,verbose=False), axis=-1)\n",
    "    y_true = int(y[0])\n",
    "    att = model_attention.predict(x,verbose=False)[0,:,:,0]\n",
    "    resized_att = resize(att, x[0].shape[:2])\n",
    "    #resizing changes values so we scale attention back to 1\n",
    "    resized_att = resized_att/np.sum(resized_att)\n",
    "\n",
    "    \n",
    "    fname_parts = Path(*Path(filename).parts[-2:])\n",
    "    parts = tifffile.imread(folder_parts/f'{fname_parts}.tiff')\n",
    "    resized_parts = resize(parts, (450,900), order=0) #everything is streched to the shape of the loaded image\n",
    "    parts_array_binarized = np.where(resized_parts > 0, 0, 1)\n",
    "\n",
    "    for p in range(0,6):\n",
    "        Attention[int(y_true), p, int(class_counter[int(y_true)])] = calculate_attention(resized_att, resized_parts, p, normalization)                \n",
    "    class_counter[int(y_true)] = class_counter[int(y_true)] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f28c8",
   "metadata": {},
   "source": [
    "### Ploting attention analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fb0e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotting per class\n",
    "for i in range(4):\n",
    "    df = pd.DataFrame({'BKGD': Attention[i, 0, :], 'Head': Attention[i, 1, :], 'Trunk': Attention[i, 2, :], 'Tail': Attention[i, 3, :], 'Yolk': Attention[i, 4, :], 'Y.E.': Attention[i, 5, :]})\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title(data_val.class_id_to_name[i])\n",
    "    plt.ylim([-0.0001, 0.005])\n",
    "\n",
    "parts = ['BKGD', 'Head', 'Trunk', 'Tail', 'Yolk', 'Yolk Extension']\n",
    "\n",
    "#plotting per fish part\n",
    "for i in range(6):\n",
    "    df = pd.DataFrame({'WT': Attention[1, i, :], 'tbx6': Attention[3, i, :], 'DAPT': Attention[0, i, :], 'her1;her7': Attention[2, i, :]})\n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title(parts[i])\n",
    "    plt.ylim([-0.0001, 0.005])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c2d87",
   "metadata": {},
   "source": [
    "### Calculating statistics for attention distribution among different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating two-sided p tests for differences in attention\n",
    "\n",
    "#significance for tail being more important in WT than her1;her7\n",
    "p = scipy.stats.ttest_ind(Attention[1,3,:], Attention[2,3,:])\n",
    "print(p.pvalue)\n",
    "#significance for tail being more important in WT than tbx6\n",
    "p = scipy.stats.ttest_ind(Attention[1,3,:], Attention[3,3,:])\n",
    "print(p.pvalue)\n",
    "#significance for tail being more important in DAPT than tbx6\n",
    "p = scipy.stats.ttest_ind(Attention[0,3,:], Attention[3,3,:])\n",
    "print(p.pvalue)\n",
    "\n",
    "#significance for yolk extension being more important in tbx6 than in the her1;her7\n",
    "p = scipy.stats.ttest_ind(Attention[3,5,:], Attention[2,5,:])\n",
    "print(p.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594144b3",
   "metadata": {},
   "source": [
    "### Analyzing the size of the first parts of different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the parts of the images that are occupied by the fish\n",
    "\n",
    "fish_occupancy = np.zeros((4,6,20))\n",
    "#0 - background\n",
    "#1 - head\n",
    "#2 - trunk\n",
    "#3 - tail \n",
    "#4 - yolk\n",
    "#5 - yolk extension\n",
    "\n",
    "#class order\n",
    "#0 - DAPT \n",
    "#1 - WT\n",
    "#2 - her1;her7\n",
    "#3 - tbx6_fss\n",
    "\n",
    "for cl_i, cl in data_test.class_id_to_name.items():\n",
    "    for i,f in tqdm(enumerate((folder_parts/cl).glob('*.tiff'))):\n",
    "        parts = tifffile.imread(f)\n",
    "        counts = np.bincount(parts.flatten())    \n",
    "        fish_occupancy[cl_i, :, i] = counts\n",
    "        fish_occupancy[cl_i, :, i] = fish_occupancy[cl_i, :, i]/parts.size\n",
    "\n",
    "        \n",
    "df_fish_occupancy = pd.DataFrame(np.transpose(np.mean(fish_occupancy, axis = 2)), columns = ['DAPT', 'WT', 'her1;her7', 'tbx6_fss'], index = ['BKGD', 'Head', 'Trunk', 'Tail', 'Yolk', 'Y.E.'])\n",
    "df_fish_occupancy = df_fish_occupancy[['WT', 'tbx6_fss', 'DAPT', 'her1;her7']]\n",
    "df_fish_occupancy = df_fish_occupancy.rename(columns = {'tbx6_fss':'tbx6'})\n",
    "#print(df_fish_occupancy['DAPT'])\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "df_fish_occupancy.plot.bar(rot=0, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
