{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning classifier for zebrafish classification\n",
    "\n",
    "This notebook is used for training of transfer learning, MobileNetV2-based classifier of zebrafish embryo mutants. The training and evaluation of the classifier is followed by Class activation mapping (CAM) analysis which points to the predictive feature of the zebrafish embryo images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from skimage.transform import resize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from sklearn import metrics\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import albumentations\n",
    "from utils import create_dataset\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the zebrafish data\n",
    "\n",
    "Download and unzip the training data (images and classes) into the subfolder `data`\n",
    "```\n",
    "wget https://zenodo.org/record/6651752/files/data.zip?download=1  -O data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "\n",
    "Which should result in the following \n",
    "\n",
    "```\n",
    "data\n",
    "├── fish_part_labels\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "├── training\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "└── validation\n",
    "    ├── DAPT\n",
    "    ├── her1;her7\n",
    "    ├── tbx6_fss\n",
    "    └── WT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot= Path('data')\n",
    "target_size = (450,900)\n",
    "\n",
    "n_tiles = 1 # splits images in n_tiles along every axis (to asses how much context matters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = create_dataset(dataroot/\"training\",   target_size=target_size, batchsize=16, shuffle=True,  augment=True, n_tiles=n_tiles)\n",
    "data_val   = create_dataset(dataroot/\"validation\", target_size=target_size, batchsize=16, shuffle=False, augment=False, n_tiles=n_tiles)\n",
    "\n",
    "print(data_train.class_name_to_id)\n",
    "print(data_train.class_id_to_name)\n",
    "\n",
    "# plot some example training and validation images\n",
    "n=5\n",
    "x,y = next(iter(data_train.data))\n",
    "plt.figure(figsize=(20,6))\n",
    "for i, (_x, _y) in enumerate(zip(x[:2*n],y[:2*n])):\n",
    "    plt.subplot(2,n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'Training (aug) - class = {data_train.class_id_to_name[int(_y)]} ({int(_y)})')\n",
    "\n",
    "x,y = next(iter(data_val.data))\n",
    "plt.figure(figsize=(20,6))\n",
    "for i, (_x, _y) in enumerate(zip(x[:2*n],y[:2*n])):\n",
    "    plt.subplot(2,n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'Validation - class = {data_train.class_id_to_name[int(_y)]} ({int(_y)})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and compiling transfer learning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp = tf.keras.Input(shape=target_size+(3,))\n",
    "model_pretrained = tf.keras.applications.MobileNetV2(input_shape = target_size+(3,), \n",
    "                                       include_top=False, weights='imagenet')\n",
    "model_pretrained.trainable = False #freeze the weights\n",
    "out = model_pretrained(inp, training=False)\n",
    "out = tf.keras.layers.GlobalAveragePooling2D()(out)\n",
    "out = tf.keras.layers.Dense(4)(out)\n",
    "model = tf.keras.Model(inp, out)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "learning_rate = 0.0003\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "#preparing the weights for balanced training\n",
    "counts = Counter(data_train.labels)\n",
    "counts_mean = np.mean(tuple(counts.values()))\n",
    "class_weights = dict((k, np.sqrt(counts_mean/v)) for k,v in counts.items())\n",
    "print(f'class weights: {class_weights}')\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00025, verbose = 1)\n",
    "\n",
    "checkpoint_folder = Path('checkpoints')\n",
    "checkpoint_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "checkpoint_filepath = checkpoint_folder/\"transfer_learning_classifier.h5\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(data_train.data.repeat(4),\n",
    "                epochs=epochs,\n",
    "                validation_data=data_val.data,\n",
    "                class_weight=class_weights, callbacks=[model_checkpoint_callback]) #reduce_lr\n",
    "np.save(checkpoint_folder/f\"transfer-learning_lr_{learning_rate:.4f}_epochs_{epochs}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[f'sparse_categorical_accuracy']\n",
    "val_acc = history.history[f'val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history[f'loss']\n",
    "val_loss = history.history[f'val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training and Validation Loss');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading weights from the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(data_val.labels)\n",
    "y_pred = np.argmax(model.predict(data_val.data), -1)\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    classes = tuple(data_val.class_id_to_name[i] for i in range(4))\n",
    "    matrix = metrics.confusion_matrix(y_true, y_pred) #rows - true, columns - predicted\n",
    "    matrix = matrix/np.sum(matrix, axis=-1, keepdims=True)\n",
    "    \n",
    "    df_cm = pd.DataFrame(matrix, index=classes, columns=classes)\n",
    "    # plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='1.3f')# font size\n",
    "    plt.ylabel(\"True class\")\n",
    "    plt.xlabel(\"Predicted class\") \n",
    "    plt.show()\n",
    "    accuracy = np.sum(np.diag(matrix))/np.sum(matrix)\n",
    "    print(f'Accuracy: {accuracy:.4f}') \n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the model to new images from a different microscope (`kings` subset)\n",
    "\n",
    "Put the `kings` images (in bmp format) in the subfolder `data/kings`, resulting in \n",
    "\n",
    "```\n",
    "data\n",
    "├── fish_part_labels\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "├── kings\n",
    "│   ├── DAPT\n",
    "│   └── WT\n",
    "├── training\n",
    "│   ├── DAPT\n",
    "│   ├── her1;her7\n",
    "│   ├── tbx6_fss\n",
    "│   └── WT\n",
    "└── validation\n",
    "    ├── DAPT\n",
    "    ├── her1;her7\n",
    "    ├── tbx6_fss\n",
    "    └── WT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = create_dataset(dataroot/'kings', augment=False, shuffle=False, target_size=target_size)\n",
    "\n",
    "x,y = next(iter(data_new.data))\n",
    "plt.figure(figsize=(20,5))\n",
    "for i, (_x, _y) in enumerate(zip(x[:2*n],y[:2*n])):\n",
    "    plt.subplot(2,n, i+1)\n",
    "    plt.imshow(np.clip(_x, 0,1))\n",
    "    plt.title(f'New image - class = {data_train.class_id_to_name[int(_y)]} ({int(_y)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(tuple(data_new.class_id_to_name[c] for c in data_new.labels))\n",
    "y_pred = np.array(tuple(data_train.class_id_to_name[c] for c in np.argmax(model.predict(data_new.data), -1)))\n",
    "\n",
    "accuracy = np.mean(y_true==y_pred)\n",
    "print(f'Accuracy: {accuracy:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (x, fname, y1, y2) in enumerate(zip(data_new.images, data_new.filenames, y_true, y_pred)):\n",
    "    if i%4==0:\n",
    "        plt.figure(figsize=(20,4))\n",
    "    ax = plt.subplot(1,4,i%4+1)\n",
    "    ax.imshow(x)\n",
    "    correct = y1==y2\n",
    "    ax.set_title(f'{\"correct\" if correct else \"wrong\"} ({y1} -> {y2})', fontsize=10)\n",
    "    ax.title.set_color('green' if correct else 'red')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from: https://nbviewer.jupyter.org/github/vincent1bt/Machine-learning-tutorials-notebooks/blob/master/activationMaps/ActivationsMaps.ipynb\n",
    "#deeper explanation here: https://vincentblog.xyz/posts/class-activation-maps\n",
    "#grad-CAM, although better than CAM, could not be used due to structure of MobileNetV2\n",
    "def get_activation_map(image, image_class): \n",
    "        #we assume image is given in a shape of [0, a, b, 3] - axb RGB image in a tupple of 1 file - this is needed so that it's compatible with predict\n",
    "        #image_class is the true class\n",
    "\n",
    "        class_weights = model.layers[-1].get_weights()[0]\n",
    "        final_conv_layer = model.layers[0].layers[152]\n",
    "        \n",
    "        get_output = tf.keras.backend.function([model.layers[0].input], \n",
    "                                               [final_conv_layer.output])\n",
    "        predictions = model.predict(image)\n",
    "        [conv_outputs] = get_output(image)\n",
    "        conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "        cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n",
    "\n",
    "        for index, weight in enumerate(class_weights[:, image_class]):\n",
    "            cam += weight * conv_outputs[:, :, index]\n",
    "        \n",
    "        class_predicted = np.argmax(predictions[0])\n",
    "        predictions = f'Class predicted: {class_predicted} | Real class: {image_class}'\n",
    "        \n",
    "        cam /= np.max(cam)\n",
    "        cam = cv2.resize(cam, (image.shape[2], image.shape[1]))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap[np.where(cam < 0.2)] = 0\n",
    "        \n",
    "        img = heatmap * 0.5 + image[0,:]\n",
    "        cv2.imwrite(\"heatmap.jpg\", img)\n",
    "        #Why is this writing and reading necessary?\n",
    "        heatmap = mpimg.imread(\"heatmap.jpg\")\n",
    "        \n",
    "        scaled_image = (((img - img.min()) * 255) / (img.max() - img.min())).astype(np.uint8)\n",
    "        \n",
    "        #puting cam into 0-1 range\n",
    "        scaled_cam = cam + np.abs(np.min(cam))\n",
    "        scaled_cam = scaled_cam/np.sum(scaled_cam) \n",
    "        \n",
    "        return scaled_cam#scaled_image\n",
    "\n",
    "path_val = Path(\"validation\")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "test_generator = test_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (int(450/N),int(900/N)), shuffle = False)\n",
    "\n",
    "#saving of the predicted images\n",
    "cam_folder = Path('CAM_results')\n",
    "if not os.path.exists(cam_folder):\n",
    "        os.mkdir(cam_folder)\n",
    "        \n",
    "classes = []\n",
    "for cl in test_generator.class_indices:\n",
    "    classes.append(cl)\n",
    "    if os.path.exists(cam_folder/cl):\n",
    "        shutil.rmtree(cam_folder/cl)\n",
    "    os.mkdir(cam_folder/cl)\n",
    "\n",
    "    \n",
    "filenames = test_generator.filenames\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = Path(\"fish_part_labels\")\n",
    "\n",
    "total = 0  \n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator.next()\n",
    "    for j in range(len(x)):\n",
    "        total = total + 1\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        image = x[j]\n",
    "        y_true = y[j].argmax()\n",
    "        #print(y_true)\n",
    "        cam = get_activation_map(np.array([image,]), y_true)\n",
    "        filename = filenames[i*20+j].split('.')[0] #batch_size = 20\n",
    "        parts_file = glob.glob(str(folder_parts/f\"{filename}*\"))\n",
    "        parts = Image.open(parts_file[0])\n",
    "        resized_parts = parts.resize((900,450), Image.NEAREST) #everything is streched to the shape of the loaded image\n",
    "        parts_array = np.array(resized_parts)\n",
    "        parts_array_binarized = np.where(parts_array > 0, 0, 1)\n",
    "        #print(filenames[i*20+j])\n",
    "        if(y_pred == y_true):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(image[0,...])   \n",
    "            plt.imshow(resize(cam,image.shape[0:2]), alpha=0.4, cmap = 'magma')\n",
    "            plt.imshow(parts_array_binarized, alpha = 0.1)\n",
    "            plt.box(False)\n",
    "            plt.axis('off')\n",
    "            #fig = resize(att[:,:,0], (450, 900))\n",
    "            plt.savefig(cam_folder/filenames[i*20+j], dpi = fig.dpi, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the per-class- and per-fish-part CAM attention for images from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(image, mask, index, normalization = 0):\n",
    "    \n",
    "    #index is the value of the part in the mask\n",
    "    #output is the sum of image pixels on positions mask == image\n",
    "    #tested on simple examples and it works as expected\n",
    "    #normalization is the parameter that define\n",
    "    \n",
    "    attention = 0\n",
    "\n",
    "    if(image.shape != mask.shape):\n",
    "        print(image.shape)\n",
    "        print(mask.shape)\n",
    "        raise Exception('Image and Mask should have the same shape')\n",
    "    \n",
    "    #I don't know if there is some trick as in Matlab to do this in two lines of code\n",
    "    n_pixels = 0\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            if(mask[i,j] == index):\n",
    "                attention = attention + image[i,j]\n",
    "                n_pixels = n_pixels + 1\n",
    "                \n",
    "    if normalization == 1:\n",
    "        attention = attention / n_pixels \n",
    "    return attention\n",
    "\n",
    "Attention = np.empty((4, 6, 20)) #4 rows are the classes, 5 colums are different zebrafish parts\n",
    "#indices for zebrafish parts: (in the array as well the values on the zebrafish part paintings)\n",
    "#0 - background\n",
    "#1 - head\n",
    "#2 - trunk\n",
    "#3 - tail \n",
    "#4 - yolk\n",
    "#5 - yolk extension\n",
    "\n",
    "#indices for classes are the same as in the classes array\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = \"fish_part_labels\"\n",
    "\n",
    "#loading the validation set\n",
    "path_val = \"validation\"\n",
    "val_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (450, 900), shuffle = False)\n",
    "#note shuffle = False in the previous line\n",
    "filenames = data_test.filenames\n",
    "\n",
    "classes = []\n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "\n",
    "class_counter = np.zeros(4) #keeping track of how many images from a certain class there were so far\n",
    "normalization = 1\n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        \n",
    "        total = total + 1\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        image = x[j]\n",
    "        \n",
    "        y_true = y[j].argmax()\n",
    "        cam = get_activation_map(np.array([image,]), y_true)\n",
    "        fig = resize(cam, (450, 900))\n",
    "        resized_cam = np.array(resize(cam, image.shape[0:2]))\n",
    "        #resizing changes values so we scale attention back to 1\n",
    "        resized_cam = resized_cam/np.sum(resized_cam)\n",
    "        \n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(att[:,:,0])\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(resized_att)\n",
    "        \n",
    "        for parts_file in glob.glob(folder_parts + '//' + filenames[i*20+j] + '*'): #batch_size = 20\n",
    "            parts = Image.open(parts_file)\n",
    "            resized_parts = parts.resize((900,450), Image.NEAREST) #everything is streched to the shape of the loaded image\n",
    "            parts_array = np.array(resized_parts)\n",
    "            for part_index in range(0,6):\n",
    "                \n",
    "                Attention[int(y_true), part_index, int(class_counter[int(y_true)])] = calculate_attention(resized_cam, parts_array, int(part_index), normalization)\n",
    "                \n",
    "        class_counter[int(y_true)] = class_counter[int(y_true)] + 1\n",
    "        #print(class_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting CAM results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting per class\n",
    "for i in range(4):\n",
    "    df = pd.DataFrame({'BKGD': Attention[i, 0, :], 'Head': Attention[i, 1, :], 'Trunk': Attention[i, 2, :], 'Tail': Attention[i, 3, :], 'Yolk': Attention[i, 4, :], 'Y.E.': Attention[i, 5, :]})\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title(classes[i])\n",
    "    plt.ylim([-0.0001, 0.0011])\n",
    "\n",
    "parts = ['BKGD', 'Head', 'Trunk', 'Tail', 'Yolk', 'Yolk Extension']\n",
    "\n",
    "#ploting per fish part\n",
    "for i in range(6):\n",
    "    df = pd.DataFrame({'WT': Attention[1, i, :], 'tbx6': Attention[3, i, :], 'DAPT': Attention[0, i, :], 'her1;her7': Attention[2, i, :]})\n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title(parts[i])\n",
    "    plt.ylim([-0.0001, 0.0011])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
