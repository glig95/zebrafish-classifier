{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trains the \"baseline\" classifier of the zebrafish embryo mutants. Baseline classifier is a shallow convolutional classifier which serves as a comparisson for the transfer learning classifier performance. The model contains 4 convolutional layers intermitted by a maxpooling layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.transform import resize\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import albumentations\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import random\n",
    "#check if GPU is visible\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = Path(\"training\")\n",
    "path_val = Path(\"validation\")\n",
    "\n",
    "AUGMENTATIONS = albumentations.Compose([albumentations.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=10, val_shift_limit=0, p=1),\n",
    "                                        albumentations.RandomBrightnessContrast(brightness_limit = (-0.3,0.3)),\n",
    "                                        albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45),\n",
    "                                        albumentations.HorizontalFlip(),\n",
    "                                        albumentations.VerticalFlip()])\n",
    "\n",
    "train_datagen_augmented = ImageDataAugmentor(rescale = 1.0/255., augment = AUGMENTATIONS, preprocess_input=None)\n",
    "val_datagen = ImageDataAugmentor( rescale = 1.0/255., preprocess_input=None) #no augmentation\n",
    "\n",
    "data_train = train_datagen_augmented.flow_from_directory(path_train, batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "\n",
    "x,y = data_train.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\", input_shape = x.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((4,4)))\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\"))\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\"))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "#another conv2D x 2 with 32\n",
    "#another max pol\n",
    "#global average pooling. kill flatt\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "#model.add(tf.keras.layers.Flatten()) \n",
    "model.add(tf.keras.layers.Dense(4,)) #from_logits = True so no need for softmax here\n",
    "\n",
    "model.summary()\n",
    "epochs = 200\n",
    "\n",
    "rate = 0.001\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=rate),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "#preparing the weights for balanced training\n",
    "counts = Counter(data_train.classes)\n",
    "counts_total = sum(counts.values())\n",
    "class_weights = dict((k, counts_total/v) for k,v in counts.items())\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,patience=2, min_lr=0.00025, verbose = 1)\n",
    "\n",
    "checkpoint_folder = Path('checkpoints')\n",
    "checkpoint_filepath = checkpoint_folder/\"baseline_classifier\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data_train,\n",
    "                epochs=epochs,\n",
    "                validation_data = data_test,\n",
    "                class_weight=class_weights, callbacks=[model_checkpoint_callback])\n",
    "np.save(checkpoint_folder/f\"baseline_classifier_lr_{rate:.4f}_epochs_{epochs}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "\n",
    "true = list() #list of true labels\n",
    "predicted = list() #list of predicted labels\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        \n",
    "        y_true = y[j]\n",
    "        \n",
    "        true.append(y_true)\n",
    "        predicted.append(y_pred)\n",
    "        \n",
    "classes = []\n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "    \n",
    "def calculate_confusion_matrix(classes, true, predicted):\n",
    "    matrix = metrics.confusion_matrix(true, predicted) #rows - true, columns - predicted\n",
    "    matrix = matrix.astype(float)\n",
    "\n",
    "\n",
    "    for i in range(len(matrix)): #scaling per row (per true label)\n",
    "        matrix[:][i] = matrix[:][i] / sum(matrix[:][i])\n",
    "    \n",
    "\n",
    "    df_cm = pd.DataFrame(matrix, index=[classes[0], classes[1], classes[2], classes[3]], columns=[classes[0], classes[1], classes[2], classes[3]])\n",
    "    # plt.figure(figsize=(10,7))\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='1.3f')# font size\n",
    "    plt.ylabel(\"True class\")\n",
    "    plt.xlabel(\"Predicted class\") \n",
    "    plt.show()\n",
    "    accuracy = sum(np.diag(matrix))/sum(sum(matrix))\n",
    "    print(accuracy) \n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_confusion_matrix(classes, true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running predictions on the test (never-seen) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val = Path(\"test\") #Folder with the test dataset\n",
    "batch_size = 16\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.) #no augmentation\n",
    "test_generator = test_datagen.flow_from_directory(path_val,  batch_size = batch_size, class_mode = 'categorical', target_size = (450,900), shuffle = False)\n",
    "\n",
    "save_folder = Path(\"test_predictions\") #Folder where images will be saved\n",
    "if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "        \n",
    "        \n",
    "classes = []\n",
    "for cl in data_train.class_indices:\n",
    "    classes.append(cl)\n",
    "    \n",
    "filenames = test_generator.filenames\n",
    "\n",
    "total = 0  \n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator.next()\n",
    "    for j in range(len(x)):\n",
    "        image = x[j]\n",
    "        \n",
    "        y_coded = y[j]\n",
    "        y_true = y_coded.argmax()\n",
    "        yhat_coded = model.predict(np.array([image,]))\n",
    "        yhat = yhat_coded.argmax()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        #plt.imshow(image)   \n",
    "\n",
    "        if(yhat == y_true):\n",
    "            plt.savefig(str(save_folder/filenames[i*batch_size+j].split('\\\\')[0]) + \"correct_as\" + classes[yhat] + \"_\" + filenames[i*batch_size+j].split('\\\\')[1], dpi = fig.dpi, transparent=True)\n",
    "        elif(yhat != y_true):\n",
    "            plt.savefig(str(save_folder/filenames[i*batch_size+j].split('\\\\')[0]) + \"wrong_as\" + classes[yhat] + \"_\"+ filenames[i*batch_size+j].split('\\\\')[1], dpi = fig.dpi, transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
