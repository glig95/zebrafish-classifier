{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used for training of transfer learning, MobileNetV2-based classifier of zebrafish embryo mutants. The training and evaluation of the classifier is followed by Class ctivation mapping (CAM) analysis which points to the predictiv feature of the zebrafish embryo images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.transform import resize\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import albumentations\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import random\n",
    "#check if GPU is visible\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = Path(\"training\")\n",
    "path_val = Path(\"validation\")\n",
    "\n",
    "\n",
    "\n",
    "AUGMENTATIONS = albumentations.Compose([albumentations.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=10, val_shift_limit=0, p=1),\n",
    "                                        albumentations.RandomBrightnessContrast(brightness_limit = (-0.3,0.3)),\n",
    "                                        albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45),\n",
    "                                        albumentations.HorizontalFlip(),\n",
    "                                        albumentations.VerticalFlip()])\n",
    "\n",
    "train_datagen_augmented = ImageDataAugmentor(rescale = 1.0/255., augment = AUGMENTATIONS, preprocess_input=None)\n",
    "val_datagen = ImageDataAugmentor( rescale = 1.0/255., preprocess_input=None) #no augmentation\n",
    "\n",
    "N = 1 #N is the number of pieces the image is cut per each dimension. For training on whole images it should be 1\n",
    "\n",
    "if(N < 15):\n",
    "    train_generator = train_datagen_augmented.flow_from_directory(path_train, batch_size = 20, class_mode = 'categorical', target_size = (int(450/N),int(900/N)))\n",
    "    validation_generator = val_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (int(450/N),int(900/N)))\n",
    "\n",
    "if(N >= 15):\n",
    "#for N = 15 it should be (30,60) but MobileNetV2 cannot accept values for any axis smaller than 32 so I redefine the size to (32,64) manually\n",
    "    train_generator = train_datagen_augmented.flow_from_directory(path_train, batch_size = 20, class_mode = 'categorical', target_size = (32,64))\n",
    "    validation_generator = val_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (32,64))\n",
    "\n",
    "x,y = train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and compiling transfer learning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape = x.shape[1:],\n",
    "                                                 include_top=False,\n",
    "                                              weights='imagenet')\n",
    "base_model.trainable = False #freeze the weights\n",
    "\n",
    "#Construct the final model\n",
    "model = tf.keras.Sequential([\n",
    "                          base_model,\n",
    "                          tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                          Dense(4, activation='softmax')])\n",
    "epochs = 200\n",
    "\n",
    "rate = 0.001\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=rate),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "#preparing the weights for balanced training\n",
    "counts = Counter(train_generator.classes)\n",
    "counts_total = sum(counts.values())\n",
    "class_weights = dict((k, counts_total/v) for k,v in counts.items())\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,patience=10, min_lr=0.00025, verbose = 1)\n",
    "checkpoint_folder = Path('checkpoints')\n",
    "checkpoint_filepath = checkpoint_folder/\"transfer_learning_classifier\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                epochs=epochs,\n",
    "                validation_data=validation_generator,\n",
    "                class_weight=class_weights, callbacks=[model_checkpoint_callback]) #reduce_lr\n",
    "np.save(checkpoint_folder/f\"transfer-learning_lr_{rate:.4f}_epochs_{epochs}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim([0,1.00])\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading weights from the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions on the validation set and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val = Path(\"validation\")\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "if(N < 15):\n",
    "    test_generator = test_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (int(450/N),int(900/N)), shuffle = False)\n",
    "if(N >= 15): \n",
    "    test_generator = test_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (32,64), shuffle = False)\n",
    "#test_generator = test_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (32,64), shuffle = False)\n",
    "\n",
    "#note shuffle = False in the previous line\n",
    "\n",
    "names = test_generator.filenames #list of names of files\n",
    "true = list() #list of true labels\n",
    "predicted = list() #list of predicted labels\n",
    "misclassified = list() #list of indices of images that are misclassified\n",
    "\n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator.next()\n",
    "    for j in range(len(x)):\n",
    "        image = x[j]\n",
    "        \n",
    "        y_coded = y[j]\n",
    "        y_true = y_coded.argmax()\n",
    "        yhat_coded = model.predict(np.array([image,]))\n",
    "        yhat = yhat_coded.argmax()\n",
    "\n",
    "        true.append(y_true)\n",
    "        predicted.append(yhat)\n",
    "        if(y_true != yhat):\n",
    "            misclassified.append(names[i*20+j]) #batch size = 20\n",
    "classes = []\n",
    "for cl in test_generator.class_indices:\n",
    "    classes.append(cl)\n",
    "\n",
    "#calculate confusion matrix\n",
    "def calculate_confusion_matrix(classes, true, predicted):\n",
    "\n",
    "\n",
    "    matrix = metrics.confusion_matrix(true, predicted) #rows - true class, columns - predicted class\n",
    "    matrix = matrix.astype(float)\n",
    "    for i in range(len(matrix)): #scaling per row (per true label)\n",
    "        matrix[:][i] = matrix[:][i] / sum(matrix[:][i])\n",
    "\n",
    "    df_cm = pd.DataFrame(matrix, index=[classes[0], classes[1], classes[2], classes[3]], columns=[classes[0], classes[1], classes[2], classes[3]])\n",
    "    sns.set(font_scale=1.4) # for label size\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "    plt.show()\n",
    "    accuracy = sum(np.diag(matrix))/sum(sum(matrix))\n",
    "    print(accuracy) \n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_confusion_matrix(classes, true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running predictions on the test (never-seen) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val = Path(\"test\") #Folder with the test dataset\n",
    "batch_size = 20\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.) #no augmentation\n",
    "if(N<15):\n",
    "    test_generator = test_datagen.flow_from_directory(path_val,  batch_size = batch_size, class_mode = 'categorical', target_size = (int(450/N),int(900/N)), shuffle = False)\n",
    "if(N >= 15):\n",
    "    test_generator = test_datagen.flow_from_directory(path_val,  batch_size = batch_size, class_mode = 'categorical', target_size = (int(32),int(64)), shuffle = False)\n",
    "\n",
    "save_folder = Path(\"test_predictions\") #Folder where images will be saved\n",
    "if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "        \n",
    "        \n",
    "classes = []\n",
    "for cl in train_generator.class_indices:\n",
    "    classes.append(cl)\n",
    "    \n",
    "filenames = test_generator.filenames\n",
    "\n",
    "total = 0  \n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator.next()\n",
    "    for j in range(len(x)):\n",
    "        image = x[j]\n",
    "        \n",
    "        y_coded = y[j]\n",
    "        y_true = y_coded.argmax()\n",
    "        yhat_coded = model.predict(np.array([image,]))\n",
    "        yhat = yhat_coded.argmax()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        #plt.imshow(image)   \n",
    "\n",
    "        if(yhat == y_true):\n",
    "            plt.savefig(str(save_folder/filenames[i*batch_size+j].split('\\\\')[0]) + \"correct_as\" + classes[yhat] + \"_\" + filenames[i*batch_size+j].split('\\\\')[1], dpi = fig.dpi, transparent=True)\n",
    "        elif(yhat != y_true):\n",
    "            plt.savefig(str(save_folder/filenames[i*batch_size+j].split('\\\\')[0]) + \"wrong_as\" + classes[yhat] + \"_\"+ filenames[i*batch_size+j].split('\\\\')[1], dpi = fig.dpi, transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from: https://nbviewer.jupyter.org/github/vincent1bt/Machine-learning-tutorials-notebooks/blob/master/activationMaps/ActivationsMaps.ipynb\n",
    "#deeper explanation here: https://vincentblog.xyz/posts/class-activation-maps\n",
    "#grad-CAM, although better than CAM, could not be used due to structure of MobileNetV2\n",
    "def get_activation_map(image, image_class): \n",
    "        #we assume image is given in a shape of [0, a, b, 3] - axb RGB image in a tupple of 1 file - this is needed so that it's compatible with predict\n",
    "        #image_class is the true class\n",
    "\n",
    "        class_weights = model.layers[-1].get_weights()[0]\n",
    "        final_conv_layer = model.layers[0].layers[152]\n",
    "        \n",
    "        get_output = tf.keras.backend.function([model.layers[0].input], \n",
    "                                               [final_conv_layer.output])\n",
    "        predictions = model.predict(image)\n",
    "        [conv_outputs] = get_output(image)\n",
    "        conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "        cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n",
    "\n",
    "        for index, weight in enumerate(class_weights[:, image_class]):\n",
    "            cam += weight * conv_outputs[:, :, index]\n",
    "        \n",
    "        class_predicted = np.argmax(predictions[0])\n",
    "        predictions = f'Class predicted: {class_predicted} | Real class: {image_class}'\n",
    "        \n",
    "        cam /= np.max(cam)\n",
    "        cam = cv2.resize(cam, (image.shape[2], image.shape[1]))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap[np.where(cam < 0.2)] = 0\n",
    "        \n",
    "        img = heatmap * 0.5 + image[0,:]\n",
    "        cv2.imwrite(\"heatmap.jpg\", img)\n",
    "        #Why is this writing and reading necessary?\n",
    "        heatmap = mpimg.imread(\"heatmap.jpg\")\n",
    "        \n",
    "        scaled_image = (((img - img.min()) * 255) / (img.max() - img.min())).astype(np.uint8)\n",
    "        \n",
    "        #puting cam into 0-1 range\n",
    "        scaled_cam = cam + np.abs(np.min(cam))\n",
    "        scaled_cam = scaled_cam/np.sum(scaled_cam) \n",
    "        \n",
    "        return scaled_cam#scaled_image\n",
    "\n",
    "path_val = Path(\"validation\")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "test_generator = test_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (int(450/N),int(900/N)), shuffle = False)\n",
    "\n",
    "#saving of the predicted images\n",
    "cam_folder = Path('CAM_results')\n",
    "if not os.path.exists(cam_folder):\n",
    "        os.mkdir(cam_folder)\n",
    "        \n",
    "classes = []\n",
    "for cl in test_generator.class_indices:\n",
    "    classes.append(cl)\n",
    "    if os.path.exists(cam_folder/cl):\n",
    "        shutil.rmtree(cam_folder/cl)\n",
    "    os.mkdir(cam_folder/cl)\n",
    "\n",
    "    \n",
    "filenames = test_generator.filenames\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = Path(\"fish_part_labels\")\n",
    "\n",
    "total = 0  \n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator.next()\n",
    "    for j in range(len(x)):\n",
    "        total = total + 1\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        image = x[j]\n",
    "        y_true = y[j].argmax()\n",
    "        #print(y_true)\n",
    "        cam = get_activation_map(np.array([image,]), y_true)\n",
    "        filename = filenames[i*20+j].split('.')[0] #batch_size = 20\n",
    "        parts_file = glob.glob(str(folder_parts/f\"{filename}*\"))\n",
    "        parts = Image.open(parts_file[0])\n",
    "        resized_parts = parts.resize((900,450), Image.NEAREST) #everything is streched to the shape of the loaded image\n",
    "        parts_array = np.array(resized_parts)\n",
    "        parts_array_binarized = np.where(parts_array > 0, 0, 1)\n",
    "        #print(filenames[i*20+j])\n",
    "        if(y_pred == y_true):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(image[0,...])   \n",
    "            plt.imshow(resize(cam,image.shape[0:2]), alpha=0.4, cmap = 'magma')\n",
    "            plt.imshow(parts_array_binarized, alpha = 0.1)\n",
    "            plt.box(False)\n",
    "            plt.axis('off')\n",
    "            #fig = resize(att[:,:,0], (450, 900))\n",
    "            plt.savefig(cam_folder/filenames[i*20+j], dpi = fig.dpi, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the per-class- and per-fish-part CAM attention for images from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(image, mask, index, normalization = 0):\n",
    "    \n",
    "    #index is the value of the part in the mask\n",
    "    #output is the sum of image pixels on positions mask == image\n",
    "    #tested on simple examples and it works as expected\n",
    "    #normalization is the parameter that define\n",
    "    \n",
    "    attention = 0\n",
    "\n",
    "    if(image.shape != mask.shape):\n",
    "        print(image.shape)\n",
    "        print(mask.shape)\n",
    "        raise Exception('Image and Mask should have the same shape')\n",
    "    \n",
    "    #I don't know if there is some trick as in Matlab to do this in two lines of code\n",
    "    n_pixels = 0\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            if(mask[i,j] == index):\n",
    "                attention = attention + image[i,j]\n",
    "                n_pixels = n_pixels + 1\n",
    "                \n",
    "    if normalization == 1:\n",
    "        attention = attention / n_pixels \n",
    "    return attention\n",
    "\n",
    "Attention = np.empty((4, 6, 20)) #4 rows are the classes, 5 colums are different zebrafish parts\n",
    "#indices for zebrafish parts: (in the array as well the values on the zebrafish part paintings)\n",
    "#0 - background\n",
    "#1 - head\n",
    "#2 - trunk\n",
    "#3 - tail \n",
    "#4 - yolk\n",
    "#5 - yolk extension\n",
    "\n",
    "#indices for classes are the same as in the classes array\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = \"fish_part_labels\"\n",
    "\n",
    "#loading the validation set\n",
    "path_val = \"validation\"\n",
    "val_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 20, class_mode = 'categorical', target_size = (450, 900), shuffle = False)\n",
    "#note shuffle = False in the previous line\n",
    "filenames = data_test.filenames\n",
    "\n",
    "classes = []\n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "\n",
    "class_counter = np.zeros(4) #keeping track of how many images from a certain class there were so far\n",
    "normalization = 1\n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        \n",
    "        total = total + 1\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        image = x[j]\n",
    "        \n",
    "        y_true = y[j].argmax()\n",
    "        cam = get_activation_map(np.array([image,]), y_true)\n",
    "        fig = resize(cam, (450, 900))\n",
    "        resized_cam = np.array(resize(cam, image.shape[0:2]))\n",
    "        #resizing changes values so we scale attention back to 1\n",
    "        resized_cam = resized_cam/np.sum(resized_cam)\n",
    "        \n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(att[:,:,0])\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(resized_att)\n",
    "        \n",
    "        for parts_file in glob.glob(folder_parts + '//' + filenames[i*20+j] + '*'): #batch_size = 20\n",
    "            parts = Image.open(parts_file)\n",
    "            resized_parts = parts.resize((900,450), Image.NEAREST) #everything is streched to the shape of the loaded image\n",
    "            parts_array = np.array(resized_parts)\n",
    "            for part_index in range(0,6):\n",
    "                \n",
    "                Attention[int(y_true), part_index, int(class_counter[int(y_true)])] = calculate_attention(resized_cam, parts_array, int(part_index), normalization)\n",
    "                \n",
    "        class_counter[int(y_true)] = class_counter[int(y_true)] + 1\n",
    "        #print(class_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting CAM results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting per class\n",
    "for i in range(4):\n",
    "    df = pd.DataFrame({'BKGD': Attention[i, 0, :], 'Head': Attention[i, 1, :], 'Trunk': Attention[i, 2, :], 'Tail': Attention[i, 3, :], 'Yolk': Attention[i, 4, :], 'Y.E.': Attention[i, 5, :]})\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title(classes[i])\n",
    "    plt.ylim([-0.0001, 0.0011])\n",
    "\n",
    "parts = ['BKGD', 'Head', 'Trunk', 'Tail', 'Yolk', 'Yolk Extension']\n",
    "\n",
    "#ploting per fish part\n",
    "for i in range(6):\n",
    "    df = pd.DataFrame({'WT': Attention[1, i, :], 'tbx6': Attention[3, i, :], 'DAPT': Attention[0, i, :], 'her1;her7': Attention[2, i, :]})\n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title(parts[i])\n",
    "    plt.ylim([-0.0001, 0.0011])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
