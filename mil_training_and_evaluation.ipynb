{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trains attention-based multiple instance learning classifier of the zerbafish embryos. The code allows training of a model with one of the three alternative backbones: MobileNetV2, ResNet50 and a shalllow convolutional network. After training, classifier's accuracy is evaluated and attention distribution is analyzed for each class. For evaluation of the attention given to the different parts of the fish, the code uses masks which denote the areas of main fish parts for images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow_datasets as tfds   \n",
    "from skimage.transform import resize\n",
    "from skimage.draw import line_nd\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import glob\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "import scipy\n",
    "from imageio import imread\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import albumentations\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import random\n",
    "#check if GPU is visible\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = Path(\"training\")\n",
    "path_val = Path(\"validation\")\n",
    "\n",
    "AUGMENTATIONS = albumentations.Compose([albumentations.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=10, val_shift_limit=0, p=1),\n",
    "                                        albumentations.RandomBrightnessContrast(brightness_limit = (-0.3,0.3)),\n",
    "                                        albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45),\n",
    "                                        albumentations.HorizontalFlip(),\n",
    "                                        albumentations.VerticalFlip()])\n",
    "\n",
    "train_datagen_augmented = ImageDataAugmentor(rescale = 1.0/255., augment = AUGMENTATIONS, preprocess_input=None)\n",
    "val_datagen = ImageDataAugmentor( rescale = 1.0/255., preprocess_input=None) #no augmentation\n",
    "\n",
    "data_train = train_datagen_augmented.flow_from_directory(path_train, batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "\n",
    "x,y = data_train.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape, backbone=\"mobilenet\"):\n",
    "    inp = tf.keras.Input(shape=shape)\n",
    "    print(backbone)\n",
    "\n",
    "    if backbone==\"mobilenet\":\n",
    "        model_pretrained = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=shape, include_top=False, weights='imagenet')\n",
    "        model_pretrained.trainable = False\n",
    "        features = model_pretrained(inp, training=False)\n",
    "        #features = model_pretrained(inp)\n",
    "    \n",
    "    elif backbone==\"resnet\":\n",
    "        model_pretrained = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=shape)\n",
    "        # get the output after the 4th round of resnet blocks, which happens to be layer 142 \n",
    "        lay_middle = [lay for lay in model_pretrained.layers if \"conv3_block4_out\" in lay.name][0]\n",
    "        model_pretrained = tf.keras.Model(model_pretrained.input, lay_middle.output)\n",
    "        features = model_pretrained(inp)\n",
    "        \n",
    "    elif backbone==\"conv\":\n",
    "        x = inp\n",
    "        for _ in range(3):\n",
    "            x = tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\")(x)\n",
    "            x = tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\")(x)\n",
    "            x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "        features = x\n",
    "    else:\n",
    "        raise NotImplementedError(backbone)\n",
    "\n",
    "    \n",
    "    features = tf.keras.layers.Conv2D(64, 1, padding=\"same\", activation=\"relu\")(features)\n",
    "\n",
    "    # attention mask\n",
    "    # two small depth wise convolutions\n",
    "\n",
    "    attention = tf.keras.layers.Conv2D(64,1, padding=\"same\", activation=\"tanh\")(features)\n",
    "    attention = tf.keras.layers.Conv2D(1,1, padding=\"same\", activation=\"linear\")(attention)\n",
    "    # make sure that it sums to one...\n",
    "    shape = attention.get_shape().as_list()        \n",
    "    attention = tf.keras.layers.Flatten()(attention)\n",
    "    attention = tf.math.softmax(attention)\n",
    "    attention = tf.keras.layers.Reshape(shape[1:])(attention)\n",
    "    # multiply by normalized attention \n",
    "    x = tf.multiply(features, attention)\n",
    "    x = tf.math.reduce_sum(x, axis=(1,2)) #attention\n",
    "    #x = tf.math.reduce_mean(features, axis=(1,2)) #no attention\n",
    "    model_attention = tf.keras.Model(inp, attention)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    out = tf.keras.layers.Dense(4,)(x)#, activation='softmax'? No, because we use \"from_logits = True\". VG\n",
    "\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    return model, model_attention\n",
    "    \n",
    "model, model_attention = build_model(x.shape[1:], backbone=\"mobilenet\")\n",
    "\n",
    "rate = 5e-4\n",
    "epochs = 100\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = Path('checkpoints')\n",
    "checkpoint_filepath = checkpoint_folder/\"MIL_classifier\"\n",
    "\n",
    "#preparing the weights for balanced training\n",
    "counts = Counter(data_train.classes)\n",
    "counts_total = sum(counts.values())\n",
    "class_weights = dict((k, counts_total/v) for k,v in counts.items())\n",
    "\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(data_train,validation_data=data_test,epochs=epochs, callbacks=[model_checkpoint_callback], class_weight = class_weights)\n",
    "np.save(checkpoint_folder/f\"MIL_lr_{rate:.4f}_epochs_{epochs}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the best-epoch weights of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_attention = build_model(x.shape[1:], backbone=\"mobilenet\")\n",
    "model.load_weights(checkpoint_filepath)\n",
    "model_attention.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading history and plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.load(checkpoint_folder/f\"MIL_lr_{rate:.4f}_epochs_{epochs}.npy\",allow_pickle='TRUE').item()\n",
    "\n",
    "acc = history['sparse_categorical_accuracy']\n",
    "val_acc = history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim([min(plt.ylim()),1.05])\n",
    "plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a few examples of the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = next(data_test)\n",
    "\n",
    "pred = np.argmax(model.predict(x), axis=-1)\n",
    "att = model_attention.predict(x)\n",
    "n_plot= 4\n",
    "plt.figure(figsize=(20,10))\n",
    "for i,(_x,_y,_p, _a) in enumerate(zip(x[:n_plot],y[:n_plot], pred[:n_plot], att[:n_plot])):\n",
    "    plt.subplot(2,n_plot,i+1)\n",
    "    plt.imshow(_x)\n",
    "    plt.title(f\"GT {_y}\")    \n",
    "    plt.subplot(2,n_plot,n_plot+i+1)\n",
    "\n",
    "    plt.imshow(_x[...,0])    \n",
    "    plt.imshow(resize(_a,_x.shape[:2])[:,:,0], alpha=.6, cmap = 'magma')\n",
    "    plt.title(f\"prediction {_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "\n",
    "true = list() #list of true labels\n",
    "predicted = list() #list of predicted labels\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        att = model_attention.predict(x[j:j+1])\n",
    "        \n",
    "        y_true = y[j]\n",
    "        \n",
    "        true.append(y_true)\n",
    "        predicted.append(y_pred)\n",
    "        \n",
    "        \n",
    "classes = []\n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "    \n",
    "def calculate_confusion_matrix(classes, true, predicted):\n",
    "    matrix = metrics.confusion_matrix(true, predicted) #rows - true, columns - predicted\n",
    "    matrix = matrix.astype(float)\n",
    "\n",
    "    accuracy = sum(np.diag(matrix))/sum(sum(matrix))\n",
    "    print(accuracy) \n",
    "\n",
    "\n",
    "    for i in range(len(matrix)): #scaling per row (per true label)\n",
    "        matrix[:][i] = matrix[:][i] / sum(matrix[:][i])\n",
    "    return matrix, accuracy\n",
    "\n",
    "[matrix, accuracy] = calculate_confusion_matrix(classes, true, predicted)\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(matrix, index=[classes[0], classes[1], classes[2], classes[3]], columns=[classes[0], classes[1], classes[2], classes[3]])\n",
    "# plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='1.3f')# font size\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating attention for all images and saving validation set images with attention overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val = Path(\"validation\")\n",
    "val_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900), shuffle = False)\n",
    "#note shuffle = False in the previous line\n",
    "output_folder = Path(\"MIL\")\n",
    "classes = []\n",
    "\n",
    "#saving of the predicted images\n",
    "if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    \n",
    "if not os.path.exists(output_folder/\"Predictions\"):\n",
    "        os.mkdir(output_folder/\"Predictions\")   \n",
    "        \n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "    if os.path.exists(output_folder/\"Predictions\"/cl):\n",
    "        shutil.rmtree(output_folder/\"Predictions\"/cl)\n",
    "    os.mkdir(output_folder/\"Predictions\"/cl)\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = Path(\"fish_part_labels\")\n",
    "filenames = data_test.filenames\n",
    "\n",
    "total = 0  \n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        total = total + 1\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        image = x[j:j+1]\n",
    "        att = model_attention.predict(image)[0,:,:,:]\n",
    "        y_true = y[j]\n",
    "        \n",
    "        filename = filenames[i*16+j].split('.')[0]\n",
    "        parts_file = glob.glob(str(folder_parts/f\"{filename}*\"))\n",
    "        parts = Image.open(parts_file[0])\n",
    "        resized_parts = parts.resize((900,450), Image.NEAREST) #everything is streched to the shape of the loaded image\n",
    "        parts_array = np.array(resized_parts)\n",
    "        parts_array_binarized = np.where(parts_array > 0, 0, 1)\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image[0,...])   \n",
    "        plt.imshow(resize(att,image.shape[1:3])[:,:,0], alpha=0.4, cmap = 'magma')\n",
    "        plt.imshow(parts_array_binarized, alpha = 0.1)\n",
    "        plt.box(False)\n",
    "        plt.axis('off')\n",
    "        #fig = resize(att[:,:,0], (450, 900)) - use this and another save funciton to plot the figure for analyzing CAMs\n",
    "        #print(filenames[i*16+j])\n",
    "        \n",
    "          \n",
    "        if(y_pred == y_true):\n",
    "            plt.savefig(output_folder/\"Predictions\"/filenames[i*16+j], dpi = fig.dpi, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running predictions on the test (never-seen) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val = Path(\"test\") #Folder with the test dataset\n",
    "batch_size = 16\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.) #no augmentation\n",
    "test_generator = test_datagen.flow_from_directory(path_val,  batch_size = batch_size, class_mode = 'categorical', target_size = (450,900), shuffle = False)\n",
    "\n",
    "save_folder = Path(\"test_predictions\") #Folder where images will be saved\n",
    "if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "        \n",
    "        \n",
    "classes = []\n",
    "for cl in data_train.class_indices:\n",
    "    classes.append(cl)\n",
    "    \n",
    "filenames = test_generator.filenames\n",
    "\n",
    "total = 0  \n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator.next()\n",
    "    for j in range(len(x)):\n",
    "        image = x[j]\n",
    "        \n",
    "        y_coded = y[j]\n",
    "        y_true = y_coded.argmax()\n",
    "        yhat_coded = model.predict(np.array([image,]))\n",
    "        yhat = yhat_coded.argmax()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        #plt.imshow(image)   \n",
    "\n",
    "        if(yhat == y_true):\n",
    "            plt.savefig(str(save_folder/filenames[i*batch_size+j].split('\\\\')[0]) + \"correct_as\" + classes[yhat] + \"_\" + filenames[i*batch_size+j].split('\\\\')[1], dpi = fig.dpi, transparent=True)\n",
    "        elif(yhat != y_true):\n",
    "            plt.savefig(str(save_folder/filenames[i*batch_size+j].split('\\\\')[0]) + \"wrong_as\" + classes[yhat] + \"_\"+ filenames[i*batch_size+j].split('\\\\')[1], dpi = fig.dpi, transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the per-class- and per-fish-part attention for images from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(image, mask, index, normalization = 0):\n",
    "    \n",
    "    #index is the value of the part in the mask\n",
    "    #output is the sum of image pixels on positions mask == image\n",
    "    #tested on simple examples and it works as expected\n",
    "    #normalization is the parameter that define\n",
    "    \n",
    "    attention = 0\n",
    "    image.shape\n",
    "    mask.shape\n",
    "    if(image.shape != mask.shape):\n",
    "        print(image.shape)\n",
    "        print(mask.shape)\n",
    "        raise Exception('Image and Mask should have the same shape')\n",
    "    \n",
    "    #I don't know if there is some trick as in Matlab to do this in two lines of code\n",
    "    n_pixels = 0\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            if(mask[i,j] == index):\n",
    "                attention = attention + image[i,j]\n",
    "                n_pixels = n_pixels + 1\n",
    "                \n",
    "    if normalization == 1:\n",
    "        attention = attention / n_pixels \n",
    "    return attention\n",
    "\n",
    "Attention = np.empty((4, 6, 20)) #4 rows are the classes, 5 colums are different zebrafish parts\n",
    "#indices for zebrafish parts: (in the array as well the values on the zebrafish part paintings)\n",
    "#0 - background\n",
    "#1 - head\n",
    "#2 - trunk\n",
    "#3 - tail \n",
    "#4 - yolk\n",
    "#5 - yolk extension\n",
    "\n",
    "#indices for classes are the same as in the classes array\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = Path(\"fish_parts_labels\")\n",
    "\n",
    "#loading the validation set\n",
    "path_val = Path(\"validation\")\n",
    "val_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900), shuffle = False)\n",
    "#note shuffle = False in the previous line\n",
    "filenames = data_test.filenames\n",
    "\n",
    "classes = []\n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "\n",
    "class_counter = np.zeros(4) #keeping track of how many images from a certain class there were so far\n",
    "normalization = 1\n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        image = x[j:j+1]\n",
    "        att = model_attention.predict(image)[0,:,:,:]\n",
    "        y_true = y[j]\n",
    "        \n",
    "        resized_att = np.array(resize(att[:,:,0], image[:2][0,:,:,0].shape))\n",
    "        #resizing changes values so we scale attention back to 1\n",
    "        resized_att = resized_att/np.sum(resized_att)\n",
    "        \n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(att[:,:,0])\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(resized_att)\n",
    "        \n",
    "        for parts_file in glob.glob(str(folder_parts/f\"{filename}*\")):\n",
    "            parts = Image.open(parts_file)\n",
    "            resized_parts = parts.resize((900,450), Image.NEAREST) #everything is streched to the shape of the loaded image\n",
    "            parts_array = np.array(resized_parts)\n",
    "            for part_index in range(0,6):\n",
    "                \n",
    "                Attention[int(y_true), part_index, int(class_counter[int(y_true)])] = calculate_attention(resized_att, parts_array, int(part_index), normalization)\n",
    "                \n",
    "        class_counter[int(y_true)] = class_counter[int(y_true)] + 1\n",
    "        #print(class_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting attention analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = ['DAPT', 'WT', 'her1;her7', 'tbx6']\n",
    "#ploting per class\n",
    "for i in range(4):\n",
    "    df = pd.DataFrame({'BGD': Attention[i, 0, :], 'Head': Attention[i, 1, :], 'Trunk': Attention[i, 2, :], 'Tail': Attention[i, 3, :], 'Yolk': Attention[i, 4, :], 'Y.E.': Attention[i, 5, :]})\n",
    "    plt.figure()\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    sns.set(font_scale=2)\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.ylim([-0.0009, 0.004])\n",
    "    plt.title(classes_[i])\n",
    "\n",
    "parts = ['BGD', 'Head', 'Trunk', 'Tail', 'Yolk', 'Yolk Extension']\n",
    "#ploting per part\n",
    "for i in range(6):\n",
    "    df = pd.DataFrame({'WT': Attention[1, i, :], 'tbx6': Attention[3, i, :], 'DAPT': Attention[0, i, :], 'her1;her7': Attention[2, i, :]})\n",
    "    plt.figure()\n",
    "    sns.set(font_scale=2)\n",
    "    ax = sns.violinplot(data = df.iloc[:, 0:6]*100, color= np.array([185, 208, 229])/255, scale='width', bw = 'scott') #we scale Y axis so that it represents percentages\n",
    "    #ax.set(ylabel=\"Attention [%]\")\n",
    "    #plt.yticks([-1, 0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.ylim([-0.0009, 0.004])\n",
    "    plt.title(parts[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating statistics for attention distribution among different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating two-sided p tests for differences in attention\n",
    "\n",
    "#significance for tail being more important in WT than her1;her7\n",
    "p = scipy.stats.ttest_ind(Attention[1,3,:], Attention[2,3,:])\n",
    "print(p.pvalue)\n",
    "#significance for tail being more important in WT than tbx6\n",
    "p = scipy.stats.ttest_ind(Attention[1,3,:], Attention[3,3,:])\n",
    "print(p.pvalue)\n",
    "#significance for tail being more important in DAPT than tbx6\n",
    "p = scipy.stats.ttest_ind(Attention[0,3,:], Attention[3,3,:])\n",
    "print(p.pvalue)\n",
    "\n",
    "#significance for yolk extension being more important in tbx6 than in the her1;her7\n",
    "p = scipy.stats.ttest_ind(Attention[3,5,:], Attention[2,5,:])\n",
    "print(p.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the size of the firs parts of different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the parts of the images that are occupied by the fish\n",
    "\n",
    "#folder with parts of the fish\n",
    "folder_parts = Path(\"fish_parts_labels\")\n",
    "\n",
    "fish_occupancy = np.zeros((4,6,20))\n",
    "#0 - background\n",
    "#1 - head\n",
    "#2 - trunk\n",
    "#3 - tail \n",
    "#4 - yolk\n",
    "#5 - yolk extension\n",
    "\n",
    "#class order\n",
    "#0 - DAPT \n",
    "#1 - WT\n",
    "#2 - her1;her7\n",
    "#3 - tbx6_fss\n",
    "\n",
    "for cl_i, cl in enumerate(data_test.class_indices):\n",
    "    for i,image in enumerate(glob.glob(folder_parts + '/' + cl + '/*')):\n",
    "        parts = np.array(Image.open(image))\n",
    "        for x in range(parts.shape[0]):\n",
    "            for y in range(parts.shape[1]):\n",
    "                pixel_value = parts[x,y]\n",
    "                fish_occupancy[cl_i, pixel_value, i] = fish_occupancy[cl_i, pixel_value, i] + 1\n",
    "        fish_occupancy[cl_i, :, i] = fish_occupancy[cl_i, :, i]/parts.size\n",
    "\n",
    "        \n",
    "df_fish_occupancy = pd.DataFrame(np.transpose(np.mean(fish_occupancy, axis = 2)), columns = ['DAPT', 'WT', 'her1;her7', 'tbx6_fss'], index = ['BKGD', 'Head', 'Trunk', 'Tail', 'Yolk', 'Y.E.'])\n",
    "df_fish_occupancy = df_fish_occupancy[['WT', 'tbx6_fss', 'DAPT', 'her1;her7']]\n",
    "df_fish_occupancy = df_fish_occupancy.rename(columns = {'tbx6_fss':'tbx6'})\n",
    "#print(df_fish_occupancy['DAPT'])\n",
    "fig = plt.figure()\n",
    "ax = df_fish_occupancy.plot.bar(rot=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
