{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trains the \"baseline\" classifier of the zebrafish embryo mutants. Baseline classifier is a shallow convolutional classifier which serves as a comparisson for the transfer learning classifier performance. The model contains 4 convolutional layers intermitted by a maxpooling layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow_datasets as tfds   \n",
    "from skimage.transform import resize\n",
    "from skimage.draw import line_nd\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import glob\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = Path(\"training\")\n",
    "path_val = Path(\"validation\")\n",
    "\n",
    "train_datagen_augmented = ImageDataGenerator(rescale = 1.0/255., brightness_range = (0.7, 1.3), rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True)\n",
    "val_datagen = ImageDataGenerator( rescale = 1.0/255. ) #no augmentation\n",
    "\n",
    "data_train = train_datagen_augmented.flow_from_directory(path_train, batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "\n",
    "x,y = data_train.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\", input_shape = x.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((4,4)))\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\"))\n",
    "model.add(tf.keras.layers.Conv2D(32,3,activation=\"relu\", padding=\"same\"))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "#another conv2D x 2 with 32\n",
    "#another max pol\n",
    "#global average pooling. kill flatt\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "#model.add(tf.keras.layers.Flatten()) \n",
    "model.add(tf.keras.layers.Dense(4,)) #from_logits = True so no need for softmax here\n",
    "\n",
    "model.summary()\n",
    "epochs = 200\n",
    "\n",
    "rate = 0.001\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=rate),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "#preparing the weights for balanced training\n",
    "counts = Counter(data_train.classes)\n",
    "counts_total = sum(counts.values())\n",
    "class_weights = dict((k, counts_total/v) for k,v in counts.items())\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,patience=2, min_lr=0.00025, verbose = 1)\n",
    "\n",
    "checkpoint_folder = Path('checkpoints')\n",
    "checkpoint_filepath = checkpoint_folder/\"baseline_classifier\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data_train,\n",
    "                epochs=epochs,\n",
    "                validation_data = data_test,\n",
    "                class_weight=class_weights, callbacks=[model_checkpoint_callback])\n",
    "np.save(checkpoint_folder/f\"baseline_classifier_lr_{rate:.4f}_epochs_{epochs}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn #importing here since it affects the plotting of the masks when imported\n",
    "data_test = val_datagen.flow_from_directory(path_val,  batch_size = 16, class_mode = 'sparse', target_size = (450, 900))\n",
    "\n",
    "true = list() #list of true labels\n",
    "predicted = list() #list of predicted labels\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    x, y = data_test.next()\n",
    "    for j in range(len(x)):\n",
    "        y_pred = np.argmax(model.predict(x[j:j+1]), axis=-1)\n",
    "        \n",
    "        y_true = y[j]\n",
    "        \n",
    "        true.append(y_true)\n",
    "        predicted.append(y_pred)\n",
    "        \n",
    "classes = []\n",
    "for cl in data_test.class_indices:\n",
    "    classes.append(cl)\n",
    "    \n",
    "def calculate_confusion_matrix(classes, true, predicted):\n",
    "    matrix = metrics.confusion_matrix(true, predicted) #rows - true, columns - predicted\n",
    "    matrix = matrix.astype(float)\n",
    "\n",
    "\n",
    "    for i in range(len(matrix)): #scaling per row (per true label)\n",
    "        matrix[:][i] = matrix[:][i] / sum(matrix[:][i])\n",
    "    \n",
    "\n",
    "    df_cm = pd.DataFrame(matrix, index=[classes[0], classes[1], classes[2], classes[3]], columns=[classes[0], classes[1], classes[2], classes[3]])\n",
    "    # plt.figure(figsize=(10,7))\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='1.3f')# font size\n",
    "    plt.ylabel(\"True class\")\n",
    "    plt.xlabel(\"Predicted class\") \n",
    "    plt.show()\n",
    "    accuracy = sum(np.diag(matrix))/sum(sum(matrix))\n",
    "    print(accuracy) \n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_confusion_matrix(classes, true, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
